{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\melis.meric.TY\\Desktop\\Sari_Meric_Ie413_Asgn2\\DocumentClassification\"\n",
    "documents = {'mis':['MIS'],'phil':['PHIL']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(fullpath):\n",
    "    for root, subfolders, filelist in os.walk(fullpath, topdown=True):\n",
    "        for currfile in filelist:\n",
    "            filepath = os.path.join(root, currfile)\n",
    "            f = io.open(filepath, encoding=\"latin-1\")\n",
    "            content = f.read()\n",
    "            # text preprocessing\n",
    "            #...\n",
    "            yield filepath, content\n",
    "def createdataframe(fullpath, label):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for fname, fcontent in readdata(fullpath):\n",
    "        rows.append({'content':fcontent, 'label':label})\n",
    "        index.append(fname)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=index)\n",
    "    return df\n",
    "    \n",
    "dataEmails = pd.DataFrame({'content':[], 'label': []})\n",
    "for label, foldernames in documents.items():\n",
    "    for foldername in foldernames:\n",
    "        documentPath = os.path.join(path, foldername)\n",
    "        dftemp = createdataframe(documentPath, label)\n",
    "        dataEmails = dataEmails.append(dftemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(C:\\Users\\melis.meric.TY\\Desktop\\Sari_Meric_Ie413_Asgn2\\DocumentClassification\\MIS\\1.txt     Economics I (3+1+0) 3 ECTS 5\\nDemand and suppl...\n",
       " C:\\Users\\melis.meric.TY\\Desktop\\Sari_Meric_Ie413_Asgn2\\DocumentClassification\\MIS\\10.txt    Business Mathematics II (3+2+0) 3 ECTS 5 \\nInd...\n",
       " Name: content, dtype: object,\n",
       " C:\\Users\\melis.meric.TY\\Desktop\\Sari_Meric_Ie413_Asgn2\\DocumentClassification\\MIS\\1.txt     mis\n",
       " C:\\Users\\melis.meric.TY\\Desktop\\Sari_Meric_Ie413_Asgn2\\DocumentClassification\\MIS\\10.txt    mis\n",
       " Name: label, dtype: object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEmails['content'][0:10], dataEmails['label'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3+1+0)\n",
      "(3+2+0)\n",
      "an\n",
      "and\n",
      "to\n",
      "of\n",
      "is\n",
      "the\n",
      "am\n",
      "I\n",
      "or\n",
      "in\n",
      "a\n",
      "by\n",
      "from\n",
      "\n",
      "frozenset({'its', 'with', 'top', 'cry', 'so', 'due', 'through', 'etc', 'no', 'almost', 'sometime', 'whereafter', 'nowhere', 'nothing', 'a', 'part', 'noone', 'him', 'eleven', 'much', 'because', 'thus', 'have', 'three', 'co', 'his', 'seem', 'one', 'mostly', 'onto', 'twelve', 'get', 'side', 'our', 'upon', 'ie', 'here', 'into', 'again', 'is', 'becomes', 'last', 'sixty', 'forty', 'under', 'serious', 'some', 'as', 'out', 'thick', 'why', 'first', 'neither', 'us', 'he', 'perhaps', 'latter', 'their', 'become', 'each', 'except', 'than', 'ours', 'but', 'all', 'move', 'whereupon', 'should', 'when', 'be', 'these', 'were', 'only', 'anyway', 'already', 'inc', 'both', 'hasnt', 'bill', 'seemed', 'in', 'fire', 'fifty', 'someone', 'beside', 'however', 'whereas', 'formerly', 'something', 'whence', 'ourselves', 'describe', 'herein', 'interest', 'whole', 'can', 'amoungst', 'also', 'i', 'we', 'amount', 'somehow', 'ever', 'after', 'could', 'please', 'where', 'amongst', 'during', 'eg', 'everyone', 'which', 'otherwise', 'same', 'any', 'you', 'her', 'hers', 'though', '(3+1+0)', 'might', 'up', 'such', 'therefore', 'further', 'everywhere', 'nor', 'well', 'fill', 'on', 'an', 'hereafter', 'anything', 'others', 'whether', 'until', 'and', 'moreover', 'take', 'beyond', 'sometimes', 'system', 'they', 'none', 'latterly', 'not', 'per', 'or', 'name', 'yourself', 'hence', 'other', 'myself', 'de', 'least', 'yet', 'twenty', 'while', 'below', 'between', 'full', 'if', 'several', 'themselves', 'towards', 'am', 'above', 'namely', 'my', 'cannot', 'became', 'now', 'un', 'every', 'at', 'hundred', 'herself', 'among', 'thereupon', 'empty', 'then', 'ltd', 'your', 'few', 'anywhere', 'wherever', 'may', 'becoming', 'across', 'them', 'thru', 'hereby', 'own', 'bottom', 'there', 'many', 'itself', 'four', 'six', 'along', 'yourselves', 'those', 'often', 'seeming', 'somewhere', 'whom', 'nine', 'whenever', 'thin', 'next', 'via', 'been', 'therein', 'will', 'how', 'toward', 'less', 'wherein', 'anyone', 'two', 'elsewhere', 'made', 'show', 'off', 'before', 'eight', 'go', 'very', 'the', 'find', 'always', 'whose', 'beforehand', 'too', 'cant', 'another', 'still', 'together', 'former', 'mine', 'himself', 'for', 'that', 'whither', 'from', 'besides', 'hereupon', 'either', 'sincere', 'within', 'nobody', 'give', 'call', 'although', 'are', 'seems', 'fifteen', 'couldnt', 'anyhow', 'yours', 'detail', 'it', 'to', 'meanwhile', 'down', 'back', 'thence', 'everything', 'mill', 'con', 'once', 'do', 'of', 're', 'who', 'whoever', 'never', 'enough', 'whatever', 'keep', 'what', 'this', 'indeed', 'nevertheless', 'being', '(3+2+0)', 'third', 'against', 'found', 'else', 'alone', 'most', 'around', 'see', 'thereafter', 'since', 'me', 'whereby', 'done', 'ten', 'by', 'behind', 'without', 'had', 'afterwards', 'over', 'must', 'thereby', 'about', 'even', 'more', 'was', 'has', 'throughout', 'front', 'put', 'she', 'would', 'rather', 'five'})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "stopFolder = open(\"StopwordsDict.txt\",\"r\")\n",
    "#print(stopFolder.read())\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"(3+1+0)\"]).union([\"(3+2+0)\"])\n",
    "#print(my_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvectorizer = CountVectorizer(ngram_range=(1,1), min_df = 1, max_features=None) \n",
    "vectorizerTFIDF = TfidfVectorizer(ngram_range=(1,1), stop_words=my_stop_words, min_df = 1, max_features=None)\n",
    "vectorizerTFIDFubgrams = TfidfVectorizer(ngram_range=(1,1), stop_words=my_stop_words, min_df = 1, max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phil    30\n",
      "mis     30\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = dataEmails[\"label\"]\n",
    "type(y)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "def evaluatemodelsbow(alldocs, y, classifiers=[], k=10, rand_state=42):\n",
    "    clfscores = dict()\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rand_state)\n",
    "    for clf in classifiers:\n",
    "        scores = []\n",
    "        for train_index, test_index in skf.split(alldocs,y):\n",
    "            X_train_docs, X_test_docs, y_train, y_test = alldocs[train_index], alldocs[test_index], y[train_index], y[test_index]\n",
    "            # Create term by document matrix with vectorizers\n",
    "            \n",
    "            # following two lines does the same as the third one\n",
    "            # X_train = vectorizerTFIDF.fit(X_train_docs)\n",
    "            # X_train = vectorizerTFIDF.transform(X_train_docs)\n",
    "            X_train = vectorizerTFIDF.fit_transform(X_train_docs)\n",
    "            X_test = vectorizerTFIDF.transform(X_test_docs)\n",
    "            \n",
    "            #X_train_tfidf_ubg = vectorizerTFIDFubgrams.fit_transform(X_train) # used for Unigram+Bigram features with TfIdf\n",
    "            #X_train_count = cvectorizer.fit_transform(X_train) # used for Unigram features with Counts\n",
    "            \n",
    "            # Train classifier\n",
    "            clf.fit(X_train, y_train)\n",
    "            # Predict test labels\n",
    "            y_pred = clf.predict(X_test)\n",
    "            # Compute the accuracy scores\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            scores.append(acc)\n",
    "        clfscores[clf] = scores\n",
    "    return clfscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melis.meric.TY\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\melis.meric.TY\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False): [1.0, 1.0, 1.0, 1.0, 1.0], DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=None, splitter='best'): [0.75, 0.9166666666666666, 0.9166666666666666, 1.0, 0.8333333333333334], RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False): [0.8333333333333334, 0.8333333333333334, 0.8333333333333334, 1.0, 1.0], MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "              validation_fraction=0.1, verbose=False, warm_start=False): [1.0, 1.0, 1.0, 1.0, 0.9166666666666666]}\n"
     ]
    }
   ],
   "source": [
    "# Classification models - create instances and then create a list of them to feed into evaluatemodelsbow function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "logreg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "mlp = MLPClassifier()\n",
    "classifiers = [logreg, dt, rf, mlp]\n",
    "alldocs = dataEmails[\"content\"]\n",
    "y = dataEmails['label']\n",
    "kfold = 5\n",
    "scores = evaluatemodelsbow(alldocs, y, classifiers, k=kfold)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phil' 'phil' 'phil' 'phil' 'phil' 'mis' 'mis' 'mis' 'mis' 'mis']\n"
     ]
    }
   ],
   "source": [
    "# UNKNOWN LABELING\n",
    "mlp = MLPClassifier() # best classifier according to k-fold CV\n",
    "tbdmatrix = vectorizerTFIDF.fit_transform(alldocs)\n",
    "mlp.fit(tbdmatrix, y)\n",
    "# read UNLABELED DOCS into unknowndocs\n",
    "dataUnknownEmails = pd.DataFrame({'content':[], 'label': []})\n",
    "folderpath4Unknowns =  r\"C:\\Users\\melis.meric.TY\\Desktop\\Sari_Meric_Ie413_Asgn2\\DocumentClassification\\UNLABELED\"\n",
    "unknowndocs = createdataframe(folderpath4Unknowns, \"UNKNOWN\")\n",
    "# transform unlabeled documents to get term by doc matrix\n",
    "tbdUnk = vectorizerTFIDF.transform(unknowndocs['content'])\n",
    "# predict the labels\n",
    "unkLabels = mlp.predict(tbdUnk)\n",
    "print(unkLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
